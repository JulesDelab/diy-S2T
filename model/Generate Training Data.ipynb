{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare audio data for image recognition\n",
    "\n",
    "The data is pretty good, but there's a few samples that aren't exactly 1 second long and some samples that are either truncated or don't contain very much of the word.\n",
    "\n",
    "The code in the notebook attempts to filter out the broken audio so that we are only using good audio.\n",
    "\n",
    "We then generate spectrograms of each word. We mix in background noise with the words to make it a more realistic audio sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data set\n",
    "Download from: https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz - approx 2.3 GB\n",
    "\n",
    "And then run\n",
    "```\n",
    "tar -xzf data_speech_commands_v0.02.tar.gz -C speech_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_io as tfio\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.io import gfile\n",
    "from tensorflow.python.ops import gen_audio_ops as audio_ops\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_DATA='speech_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The audio is all sampled at 16KHz and should all be 1 second in length - so 1 second is 16000 samples\n",
    "EXPECTED_SAMPLES=16000\n",
    "# Noise floor to detect if any audio is present\n",
    "NOISE_FLOOR=0.1\n",
    "# How many samples should be abover the noise floor?\n",
    "MINIMUM_VOICE_LENGTH=EXPECTED_SAMPLES/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of folders we want to process in the speech_data folder\n",
    "from tensorflow.python.ops import gen_audio_ops as audio_ops\n",
    "words = [\n",
    "    'backward',\n",
    "    'bed',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'dog',\n",
    "    'down',\n",
    "    'eight',\n",
    "    'five',\n",
    "    'follow',\n",
    "    'forward',\n",
    "    'four',\n",
    "    'go',\n",
    "    'happy',\n",
    "    'house',\n",
    "    'learn',\n",
    "    'left',\n",
    "    'marvin',\n",
    "    'nine',\n",
    "    'no',\n",
    "    'off',\n",
    "    'on',\n",
    "    'one',\n",
    "    'right',\n",
    "    'seven',\n",
    "    'sheila',\n",
    "    'six',\n",
    "    'stop',\n",
    "    'three',\n",
    "    'tree',\n",
    "    'two',\n",
    "    'up',\n",
    "    'visual',\n",
    "    'wow',\n",
    "    'yes',\n",
    "    'zero',\n",
    "    '_background',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the files in a directory\n",
    "def get_files(word):\n",
    "    return gfile.glob(SPEECH_DATA + '/'+word+'/*.wav')\n",
    "\n",
    "# get the location of the voice\n",
    "def get_voice_position(audio, noise_floor):\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return tfio.audio.trim(audio, axis=0, epsilon=noise_floor)\n",
    "\n",
    "# Work out how much of the audio file is actually voice\n",
    "def get_voice_length(audio, noise_floor):\n",
    "    position = get_voice_position(audio, noise_floor)\n",
    "    return (position[1] - position[0]).numpy()\n",
    "\n",
    "# is enough voice present?\n",
    "def is_voice_present(audio, noise_floor, required_length):\n",
    "    voice_length = get_voice_length(audio, noise_floor)\n",
    "    return voice_length >= required_length\n",
    "\n",
    "# is the audio the correct length?\n",
    "def is_correct_length(audio, expected_length):\n",
    "    return (audio.shape[0]==expected_length).numpy()\n",
    "\n",
    "\n",
    "def is_valid_file(file_name):\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    # check the file is long enough\n",
    "    if not is_correct_length(audio_tensor, EXPECTED_SAMPLES):\n",
    "        return False\n",
    "    # convert the audio to an array of floats and scale it to betweem -1 and 1\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    # is there any voice in the audio?\n",
    "    if not is_voice_present(audio, NOISE_FLOOR, MINIMUM_VOICE_LENGTH):\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(audio):\n",
    "    # normalise the audio\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    # create the spectrogram\n",
    "    spectrogram = audio_ops.audio_spectrogram(audio,\n",
    "                                              window_size=320,\n",
    "                                              stride=160,\n",
    "                                              magnitude_squared=True).numpy()\n",
    "    # reduce the number of frequency bins in our spectrogram to a more sensible level\n",
    "    spectrogram = tf.nn.pool(\n",
    "        input=tf.expand_dims(spectrogram, -1),\n",
    "        window_shape=[1, 6],\n",
    "        strides=[1, 6],\n",
    "        pooling_type='AVG',\n",
    "        padding='SAME')\n",
    "    spectrogram = tf.squeeze(spectrogram, axis=0)\n",
    "    spectrogram = np.log10(spectrogram + 1e-6)\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process a file into its spectrogram\n",
    "def process_file(file_path):\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_path)\n",
    "    # convert the audio to an array of floats and scale it to betweem -1 and 1\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    # randomly reposition the audio in the sample\n",
    "    voice_start, voice_end = get_voice_position(audio, NOISE_FLOOR)\n",
    "    end_gap=len(audio) - voice_end\n",
    "    random_offset = np.random.uniform(0, voice_start+end_gap)\n",
    "    audio = np.roll(audio,-random_offset+end_gap)\n",
    "    # add some random background noise\n",
    "    background_volume = np.random.uniform(0, 0.1)\n",
    "    # get the background noise files\n",
    "    background_files = get_files('_background_noise_')\n",
    "    background_file = np.random.choice(background_files)\n",
    "    background_tensor = tfio.audio.AudioIOTensor(background_file)\n",
    "    background_start = np.random.randint(0, len(background_tensor) - 16000)\n",
    "    # normalise the background noise\n",
    "    background = tf.cast(background_tensor[background_start:background_start+16000], tf.float32)\n",
    "    background = background - np.mean(background)\n",
    "    background = background / np.max(np.abs(background))\n",
    "    # mix the audio with the scaled background\n",
    "    audio = audio + background_volume * background\n",
    "    # get the spectrogram\n",
    "    return get_spectrogram(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "validate = []\n",
    "test = []\n",
    "\n",
    "TRAIN_SIZE=0.8\n",
    "VALIDATION_SIZE=0.1\n",
    "TEST_SIZE=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2100191dc2f946bcb93f5dd20dcfd00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing words:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42dc92441534ebab986eb2130de446a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking:   0%|          | 0/1664 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "unable to open file: libtensorflow_io.so, from paths: ['c:\\\\users\\\\jules\\\\projects\\\\diy-s2t\\\\model\\\\venv\\\\lib\\\\site-packages\\\\tensorflow_io\\\\python\\\\ops\\\\libtensorflow_io.so']\ncaused by: ['c:\\\\users\\\\jules\\\\projects\\\\diy-s2t\\\\model\\\\venv\\\\lib\\\\site-packages\\\\tensorflow_io\\\\python\\\\ops\\\\libtensorflow_io.so not found']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m word:\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;66;03m# add more examples of marvin to balance our training set\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         repeat \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarvin\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 47\u001b[0m         \u001b[43mprocess_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train), \u001b[38;5;28mlen\u001b[39m(test), \u001b[38;5;28mlen\u001b[39m(validate))\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36mprocess_word\u001b[1;34m(word, repeat)\u001b[0m\n\u001b[0;32m      8\u001b[0m label \u001b[38;5;241m=\u001b[39m words\u001b[38;5;241m.\u001b[39mindex(word)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# get a list of files names for the word\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m file_names \u001b[38;5;241m=\u001b[39m [file_name \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m tqdm(get_files(word), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m is_valid_file(file_name)]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# randomly shuffle the filenames\u001b[39;00m\n\u001b[0;32m     12\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(file_names)\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m label \u001b[38;5;241m=\u001b[39m words\u001b[38;5;241m.\u001b[39mindex(word)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# get a list of files names for the word\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m file_names \u001b[38;5;241m=\u001b[39m [file_name \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m tqdm(get_files(word), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_valid_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# randomly shuffle the filenames\u001b[39;00m\n\u001b[0;32m     12\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(file_names)\n",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m, in \u001b[0;36mis_valid_file\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_valid_file\u001b[39m(file_name):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# load the audio file\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     audio_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtfio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAudioIOTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# check the file is long enough\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_correct_length(audio_tensor, EXPECTED_SAMPLES):\n",
      "File \u001b[1;32mc:\\users\\jules\\projects\\diy-s2t\\model\\venv\\lib\\site-packages\\tensorflow_io\\python\\ops\\audio_ops.py:664\u001b[0m, in \u001b[0;36mAudioIOTensor.__init__\u001b[1;34m(self, filename, dtype)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype must be provided in graph mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 664\u001b[0m resource \u001b[38;5;241m=\u001b[39m \u001b[43mcore_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio_audio_readable_init\u001b[49m(filename)\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    666\u001b[0m     shape, dtype, rate \u001b[38;5;241m=\u001b[39m core_ops\u001b[38;5;241m.\u001b[39mio_audio_readable_spec(resource)\n",
      "File \u001b[1;32mc:\\users\\jules\\projects\\diy-s2t\\model\\venv\\lib\\site-packages\\tensorflow_io\\python\\ops\\__init__.py:88\u001b[0m, in \u001b[0;36mLazyLoader.__getattr__\u001b[1;34m(self, attrb)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attrb):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, attrb)\n",
      "File \u001b[1;32mc:\\users\\jules\\projects\\diy-s2t\\model\\venv\\lib\\site-packages\\tensorflow_io\\python\\ops\\__init__.py:84\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mod \u001b[38;5;241m=\u001b[39m \u001b[43m_load_library\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_library\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mod\n",
      "File \u001b[1;32mc:\\users\\jules\\projects\\diy-s2t\\model\\venv\\lib\\site-packages\\tensorflow_io\\python\\ops\\__init__.py:69\u001b[0m, in \u001b[0;36m_load_library\u001b[1;34m(filename, lib)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mNotFoundError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     68\u001b[0m         errs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munable to open file: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, from paths: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilenames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mcaused by: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: unable to open file: libtensorflow_io.so, from paths: ['c:\\\\users\\\\jules\\\\projects\\\\diy-s2t\\\\model\\\\venv\\\\lib\\\\site-packages\\\\tensorflow_io\\\\python\\\\ops\\\\libtensorflow_io.so']\ncaused by: ['c:\\\\users\\\\jules\\\\projects\\\\diy-s2t\\\\model\\\\venv\\\\lib\\\\site-packages\\\\tensorflow_io\\\\python\\\\ops\\\\libtensorflow_io.so not found']"
     ]
    }
   ],
   "source": [
    "def process_files(file_names, label, repeat=1):\n",
    "    file_names = tf.repeat(file_names, repeat).numpy()\n",
    "    return [(process_file(file_name), label) for file_name in tqdm(file_names, desc=f\"{word} ({label})\", leave=False)]\n",
    "\n",
    "# process the files for a word into the spectrogram and one hot encoding word value\n",
    "def process_word(word, repeat=1):\n",
    "    # the index of the word word we are processing\n",
    "    label = words.index(word)\n",
    "    # get a list of files names for the word\n",
    "    file_names = [file_name for file_name in tqdm(get_files(word), desc=\"Checking\", leave=False) if is_valid_file(file_name)]\n",
    "    # randomly shuffle the filenames\n",
    "    np.random.shuffle(file_names)\n",
    "    # split the files into train, validate and test buckets\n",
    "    train_size=int(TRAIN_SIZE*len(file_names))\n",
    "    validation_size=int(VALIDATION_SIZE*len(file_names))\n",
    "    test_size=int(TEST_SIZE*len(file_names))\n",
    "    # get the training samples\n",
    "    train.extend(\n",
    "        process_files(\n",
    "            file_names[:train_size],\n",
    "            label,\n",
    "            repeat=repeat\n",
    "        )\n",
    "    )\n",
    "    # and the validation samples\n",
    "    validate.extend(\n",
    "        process_files(\n",
    "            file_names[train_size:train_size+validation_size],\n",
    "            label,\n",
    "            repeat=repeat\n",
    "        )\n",
    "    )\n",
    "    # and the test samples\n",
    "    test.extend(\n",
    "        process_files(\n",
    "            file_names[train_size+validation_size:],\n",
    "            label,\n",
    "            repeat=repeat\n",
    "        )\n",
    "    )\n",
    "\n",
    "# process all the words and all the files\n",
    "for word in tqdm(words, desc=\"Processing words\"):\n",
    "    if '_' not in word:\n",
    "        # add more examples of marvin to balance our training set\n",
    "        repeat = 70 if word == 'marvin' else 1\n",
    "        process_word(word, repeat=repeat)\n",
    "    \n",
    "print(len(train), len(test), len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the background noise files\n",
    "def process_background(file_name, label):\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio_length = len(audio)\n",
    "    samples = []\n",
    "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 8000), desc=file_name, leave=False):\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = audio[section_start:section_end]\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(section)\n",
    "        samples.append((spectrogram, label))\n",
    "\n",
    "    # simulate random utterances\n",
    "    for section_index in tqdm(range(1000), desc=\"Simulated Words\", leave=False):\n",
    "        section_start = np.random.randint(0, audio_length - EXPECTED_SAMPLES)\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = np.reshape(audio[section_start:section_end], (EXPECTED_SAMPLES))\n",
    "\n",
    "        result = np.zeros((EXPECTED_SAMPLES))\n",
    "        # create a pseudo bit of voice\n",
    "        voice_length = np.random.randint(MINIMUM_VOICE_LENGTH/2, EXPECTED_SAMPLES)\n",
    "        voice_start = np.random.randint(0, EXPECTED_SAMPLES - voice_length)\n",
    "        hamming = np.hamming(voice_length)\n",
    "        # amplify the voice section\n",
    "        result[voice_start:voice_start+voice_length] = hamming * section[voice_start:voice_start+voice_length]\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(np.reshape(section, (16000, 1)))\n",
    "        samples.append((spectrogram, label))\n",
    "        \n",
    "    \n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    train_size=int(TRAIN_SIZE*len(samples))\n",
    "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
    "    test_size=int(TEST_SIZE*len(samples))\n",
    "    \n",
    "    train.extend(samples[:train_size])\n",
    "\n",
    "    validate.extend(samples[train_size:train_size+validation_size])\n",
    "\n",
    "    test.extend(samples[train_size+validation_size:])\n",
    "\n",
    "        \n",
    "for file_name in tqdm(get_files('_background_noise_'), desc=\"Processing Background Noise\"):\n",
    "    process_background(file_name, words.index(\"_background\"))\n",
    "    \n",
    "print(len(train), len(test), len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_problem_noise(file_name, label):\n",
    "    samples = []\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio_length = len(audio)\n",
    "    samples = []\n",
    "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 400), desc=file_name, leave=False):\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = audio[section_start:section_end]\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(section)\n",
    "        samples.append((spectrogram, label))\n",
    "        \n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    train_size=int(TRAIN_SIZE*len(samples))\n",
    "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
    "    test_size=int(TEST_SIZE*len(samples))\n",
    "    \n",
    "    train.extend(samples[:train_size])\n",
    "    validate.extend(samples[train_size:train_size+validation_size])\n",
    "    test.extend(samples[train_size+validation_size:])\n",
    "\n",
    "\n",
    "for file_name in tqdm(get_files(\"_problem_noise_\"), desc=\"Processing problem noise\"):\n",
    "    process_problem_noise(file_name, words.index(\"_background\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mar_sounds(file_name, label):\n",
    "    samples = []\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio_length = len(audio)\n",
    "    samples = []\n",
    "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 4000), desc=file_name, leave=False):\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = audio[section_start:section_end]\n",
    "        section = section - np.mean(section)\n",
    "        section = section / np.max(np.abs(section))\n",
    "        # add some random background noise\n",
    "        background_volume = np.random.uniform(0, 0.1)\n",
    "        # get the background noise files\n",
    "        background_files = get_files('_background_noise_')\n",
    "        background_file = np.random.choice(background_files)\n",
    "        background_tensor = tfio.audio.AudioIOTensor(background_file)\n",
    "        background_start = np.random.randint(0, len(background_tensor) - 16000)\n",
    "        # normalise the background noise\n",
    "        background = tf.cast(background_tensor[background_start:background_start+16000], tf.float32)\n",
    "        background = background - np.mean(background)\n",
    "        background = background / np.max(np.abs(background))\n",
    "        # mix the audio with the scaled background\n",
    "        section = section + background_volume * background\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(section)\n",
    "        samples.append((spectrogram, label))\n",
    "        \n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    train_size=int(TRAIN_SIZE*len(samples))\n",
    "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
    "    test_size=int(TEST_SIZE*len(samples))\n",
    "    \n",
    "    train.extend(samples[:train_size])\n",
    "    validate.extend(samples[train_size:train_size+validation_size])\n",
    "    test.extend(samples[train_size+validation_size:])\n",
    "\n",
    "\n",
    "for file_name in tqdm(get_files(\"_mar_sounds_\"), desc=\"Processing problem noise\"):\n",
    "    process_mar_sounds(file_name, words.index(\"_background\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(train), len(test), len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomise the training samples\n",
    "np.random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = zip(*train)\n",
    "X_validate, Y_validate = zip(*validate)\n",
    "X_test, Y_test = zip(*test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the computed data\n",
    "np.savez_compressed(\n",
    "    \"training_spectrogram.npz\",\n",
    "    X=X_train, Y=Y_train)\n",
    "print(\"Saved training data\")\n",
    "np.savez_compressed(\n",
    "    \"validation_spectrogram.npz\",\n",
    "    X=X_validate, Y=Y_validate)\n",
    "print(\"Saved validation data\")\n",
    "np.savez_compressed(\n",
    "    \"test_spectrogram.npz\",\n",
    "    X=X_test, Y=Y_test)\n",
    "print(\"Saved test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the width and height of the spectrogram \"image\"\n",
    "IMG_WIDTH=X_train[0].shape[0]\n",
    "IMG_HEIGHT=X_train[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images2(images_arr, imageWidth, imageHeight):\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(10, 20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(np.reshape(img, (imageWidth, imageHeight)))\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = words.index(\"marvin\")\n",
    "\n",
    "X_marvins = np.array(X_train)[np.array(Y_train) == word_index]\n",
    "Y_marvins = np.array(Y_train)[np.array(Y_train) == word_index]\n",
    "plot_images2(X_marvins[:20], IMG_WIDTH, IMG_HEIGHT)\n",
    "print(Y_marvins[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = words.index(\"yes\")\n",
    "\n",
    "X_yes = np.array(X_train)[np.array(Y_train) == word_index]\n",
    "Y_yes = np.array(Y_train)[np.array(Y_train) == word_index]\n",
    "plot_images2(X_yes[:20], IMG_WIDTH, IMG_HEIGHT)\n",
    "print(Y_yes[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
